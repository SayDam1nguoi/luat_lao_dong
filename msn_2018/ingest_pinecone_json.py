# ===================== IMPORTS =====================
import os
import time
import json
from typing import List, Dict, Any

from dotenv import load_dotenv
load_dotenv(override=True)

from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import Pinecone
from pinecone import Pinecone as PineconeClient, PodSpec


# ===================== Cáº¤U HÃŒNH =====================
OPENAI_API_KEY = os.getenv("OPENAI__API_KEY")
OPENAI_EMBEDDING_MODEL = os.getenv("OPENAI__EMBEDDING_MODEL")

PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENVIRONMENT = os.getenv("PINECONE_ENVIRONMENT")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME_MSN_2018")

EMBEDDING_DIM = 3072
JSON_FOLDER = r"C:\Users\tabao\OneDrive\Desktop\cong_viec_lam\data_msn_2018"
BATCH_SIZE = 30


# ===================== INIT =====================
print("ğŸ”§ Khá»Ÿi táº¡o Pinecone & Embedding...")

if not all([
    OPENAI_API_KEY,
    OPENAI_EMBEDDING_MODEL,
    PINECONE_API_KEY,
    PINECONE_ENVIRONMENT,
    PINECONE_INDEX_NAME
]):
    raise RuntimeError("âŒ Thiáº¿u biáº¿n mÃ´i trÆ°á»ng")

pc = PineconeClient(api_key=PINECONE_API_KEY)

emb = OpenAIEmbeddings(
    api_key=OPENAI_API_KEY,
    model=OPENAI_EMBEDDING_MODEL
)

print("âœ… Sáºµn sÃ ng\n")


# ===================== UTIL =====================
def get_json_files_from_folder(folder: str) -> List[str]:
    if not os.path.exists(folder):
        return []
    return sorted(
        os.path.join(folder, f)
        for f in os.listdir(folder)
        if f.lower().endswith(".json")
    )


def create_or_get_index(index_name: str, force: bool = False):
    if force and index_name in pc.list_indexes().names():
        print(f"ğŸ—‘ï¸ XÃ³a index {index_name}")
        pc.delete_index(index_name)
        time.sleep(3)

    if index_name not in pc.list_indexes().names():
        print(f"ğŸ› ï¸ Táº¡o index {index_name}")
        pc.create_index(
            name=index_name,
            dimension=EMBEDDING_DIM,
            metric="cosine",
            spec=PodSpec(environment=PINECONE_ENVIRONMENT)
        )
        time.sleep(5)

    return pc.Index(index_name)


# ===================== VSIC HELPERS =====================
def detect_level(code: str) -> str:
    """
    XÃ¡c Ä‘á»‹nh cáº¥p VSIC dá»±a vÃ o mÃ£
    """
    if not code:
        return "unknown"

    if code.isalpha():
        return "section"      # A, B, C

    if code.isdigit():
        if len(code) == 2:
            return "division"     # 01
        if len(code) == 3:
            return "group"        # 011
        if len(code) == 4:
            return "class"        # 0118
        if len(code) == 5:
            return "subclass"     # 01110

    return "unknown"


# ===================== LOAD JSON (MAPPING) =====================
def load_and_chunk_json(file_path: str) -> List[Dict[str, Any]]:
    """
    JSON dáº¡ng mapping pháº³ng:
    {
        "A": "NÃ”NG NGHIá»†P, LÃ‚M NGHIá»†P VÃ€ THUá»¶ Sáº¢N",
        "01": "NÃ´ng nghiá»‡p vÃ  hoáº¡t Ä‘á»™ng dá»‹ch vá»¥ cÃ³ liÃªn quan",
        "01110": "Trá»“ng lÃºa",
        ...
    }
    """
    filename = os.path.basename(file_path)

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        if not isinstance(data, dict):
            print(f"âš ï¸ {filename} khÃ´ng pháº£i JSON object")
            return []

        docs: List[Dict[str, Any]] = []

        for code, name in data.items():
            if not isinstance(name, str):
                continue

            name_clean = name.strip()
            if not name_clean:
                continue

            text = f"MÃ£ ngÃ nh {code}: {name_clean}"

            docs.append({
                "text": text,
                "metadata": {
                    "industry_code": code,
                    "industry_name": name_clean,
                    "level": detect_level(code),
                    "source_file": filename
                }
            })

        return docs

    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c JSON {filename}: {e}")
        return []


# ===================== INGEST =====================
def ingest_documents_to_pinecone(
    json_paths: List[str],
    index_name: str,
    force_reload: bool = False
):
    print("=" * 70)
    print("ğŸš€ INGEST VSIC JSON â†’ PINECONE")
    print("=" * 70)
    print(f"ğŸ“ Folder: {JSON_FOLDER}")
    print(f"ğŸ“š File JSON: {len(json_paths)}")
    print(f"â˜ï¸ Index: {index_name}\n")

    index = create_or_get_index(index_name, force_reload)

    all_docs: List[Dict[str, Any]] = []
    file_stats: Dict[str, int] = {}

    print("ğŸ“– Load JSON...\n")

    for path in json_paths:
        filename = os.path.basename(path)
        print(f"ğŸ“„ {filename}...", end=" ")

        docs = load_and_chunk_json(path)
        if not docs:
            print("âœ—")
            continue

        all_docs.extend(docs)
        file_stats[filename] = len(docs)
        print(f"âœ“ {len(docs)} entries")

    if not all_docs:
        raise RuntimeError("âŒ KhÃ´ng cÃ³ document Ä‘á»ƒ ingest")

    print(f"\nğŸ“¦ Tá»•ng vectors: {len(all_docs)}")
    print("ğŸ’¾ Náº¡p Pinecone...\n")

    vectordb = None
    total_batches = (len(all_docs) + BATCH_SIZE - 1) // BATCH_SIZE

    for i in range(0, len(all_docs), BATCH_SIZE):
        batch = all_docs[i:i + BATCH_SIZE]
        batch_no = (i // BATCH_SIZE) + 1

        print(f"   ğŸ“¦ Batch {batch_no}/{total_batches} ({len(batch)} docs)...", end=" ")

        texts = [d["text"] for d in batch]
        metadatas = [d["metadata"] for d in batch]

        if vectordb is None:
            vectordb = Pinecone.from_texts(
                texts=texts,
                metadatas=metadatas,
                embedding=emb,
                index_name=index_name
            )
        else:
            vectordb.add_texts(
                texts=texts,
                metadatas=metadatas
            )

        print("âœ“")
        time.sleep(0.5)

    stats = index.describe_index_stats()

    print("\n" + "=" * 70)
    print("ğŸ“Š Káº¾T QUáº¢")
    print("=" * 70)
    print(f"âœ… Tá»•ng vectors trong index: {stats['total_vector_count']}")
    print(f"ğŸ“ File xá»­ lÃ½: {len(file_stats)}")
    for f, c in file_stats.items():
        print(f"   â€¢ {f}: {c} vectors")
    print("=" * 70)


# ===================== MAIN =====================
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser("Ingest VSIC JSON â†’ Pinecone")
    parser.add_argument("--force-reload", action="store_true")
    parser.add_argument("--folder", type=str, default=JSON_FOLDER)

    args = parser.parse_args()

    json_files = get_json_files_from_folder(args.folder)

    if not json_files:
        raise RuntimeError("âŒ KhÃ´ng tÃ¬m tháº¥y file JSON")

    print(f"ğŸ“„ TÃ¬m tháº¥y {len(json_files)} file JSON:")
    for i, f in enumerate(json_files, 1):
        print(f"   {i}. {os.path.basename(f)}")
    print()

    ingest_documents_to_pinecone(
        json_paths=json_files,
        index_name=PINECONE_INDEX_NAME,
        force_reload=args.force_reload
    )

    print("\nğŸ‰ HOÃ€N THÃ€NH")
