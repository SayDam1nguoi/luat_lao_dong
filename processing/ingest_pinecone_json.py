# ===================== IMPORTS =====================
import os
import time
import json
from typing import List, Dict, Any

from dotenv import load_dotenv
load_dotenv(override=True)

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import Pinecone 
from pinecone import Pinecone as PineconeClient, PodSpec

# ===================== Cáº¤U HÃŒNH =====================
OPENAI_API_KEY = os.getenv("OPENAI__API_KEY")
OPENAI_EMBEDDING_MODEL = os.getenv("OPENAI__EMBEDDING_MODEL")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENVIRONMENT = os.getenv("PINECONE_ENVIRONMENT")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME")

EMBEDDING_DIM = 3072  
JSON_FOLDER = r"C:\Users\tabao\OneDrive\Desktop\cong_viec_lam\json"
BATCH_SIZE = 30  

# ===================== KHá»I Táº O =====================
print("ğŸ”§ Äang khá»Ÿi táº¡o Pinecone Client vÃ  Embedding...")

if not all([OPENAI_API_KEY, PINECONE_API_KEY, PINECONE_ENVIRONMENT, PINECONE_INDEX_NAME]):
    print("âŒ Lá»–I: Thiáº¿u biáº¿n mÃ´i trÆ°á»ng báº¯t buá»™c!")
    exit(1)

pc = PineconeClient(api_key=PINECONE_API_KEY)
emb = OpenAIEmbeddings(api_key=OPENAI_API_KEY, model=OPENAI_EMBEDDING_MODEL)

print("âœ… ÄÃ£ khá»Ÿi táº¡o thÃ nh cÃ´ng!\n")

# ===================== HÃ€M Há»– TRá»¢ =====================

def get_json_files_from_folder(folder_path: str) -> List[str]:
    """Láº¥y táº¥t cáº£ file JSON trong folder."""
    if not os.path.exists(folder_path):
        print(f"âš ï¸ Folder khÃ´ng tá»“n táº¡i: {folder_path}")
        return []
    
    json_files = []
    for file in os.listdir(folder_path):
        if file.lower().endswith(".json"):
            json_files.append(os.path.join(folder_path, file))
    
    return sorted(json_files)


def get_existing_sources_from_index(index_name: str) -> set:
    """Láº¥y danh sÃ¡ch file Ä‘Ã£ cÃ³ trong Index."""
    try:
        if index_name not in pc.list_indexes().names():
            return set()
        
        index = pc.Index(index_name)
        stats = index.describe_index_stats()
        
        if stats["total_vector_count"] == 0:
            return set()

        dummy_query = [0.0] * EMBEDDING_DIM
        results = index.query(
            vector=dummy_query,
            top_k=50,
            include_metadata=True
        )
        
        sources = set()
        for match in results.get("matches", []):
            if "metadata" in match and "source" in match["metadata"]:
                sources.add(match["metadata"]["source"])
        
        return sources
    
    except Exception as e:
        print(f"âš ï¸ Lá»—i khi láº¥y danh sÃ¡ch file tá»« Index: {e}")
        return set()


def create_or_get_index(index_name: str, force_recreate: bool = False):
    """Táº¡o hoáº·c láº¥y Pinecone Index."""
    
    if force_recreate:
        print(f"ğŸ—‘ï¸ Äang xÃ³a Index '{index_name}' (náº¿u tá»“n táº¡i)...")
        if index_name in pc.list_indexes().names():
            pc.delete_index(index_name)
            print(f"âœ… ÄÃ£ xÃ³a Index '{index_name}'")
            time.sleep(3)

    if index_name not in pc.list_indexes().names():
        print(f"ğŸ› ï¸ Äang táº¡o Index '{index_name}'...")
        pc.create_index(
            name=index_name,
            dimension=EMBEDDING_DIM,
            metric="cosine",
            spec=PodSpec(environment=PINECONE_ENVIRONMENT)
        )
        print(f"âœ… ÄÃ£ táº¡o Index '{index_name}'")
        time.sleep(5)

    return pc.Index(index_name)


def load_and_chunk_json(file_path: str) -> List[Dict[str, Any]]:
    """Äá»c file JSON vÃ  táº¡o cÃ¡c document Ä‘á»ƒ náº¡p vÃ o Pinecone."""
    filename = os.path.basename(file_path)

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        docs = []
        for code, desc in data.items():
            text = f"{code}: {desc}"

            docs.append({
                "text": text,
                "metadata": {
                    "source": filename,
                    "code": code
                }
            })

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,
            chunk_overlap=0
        )

        final_docs = []
        for doc in docs:
            chunks = splitter.split_text(doc["text"])
            for i, chunk in enumerate(chunks):
                final_docs.append({
                    "text": chunk,
                    "metadata": {**doc["metadata"], "chunk_id": i}
                })

        return final_docs

    except Exception as e:
        print(f"âŒ Lá»—i khi load JSON {filename}: {e}")
        return []


def ingest_documents_to_pinecone(
    json_paths: List[str],
    index_name: str,
    force_reload: bool = False
):
    print("="*70)
    print("ğŸš€ Báº®T Äáº¦U Náº P Dá»® LIá»†U JSON VÃ€O PINECONE")
    print("="*70)
    print(f"ğŸ“ Folder: {JSON_FOLDER}")
    print(f"ğŸ“š Tá»•ng sá»‘ file JSON: {len(json_paths)}")
    print(f"â˜ï¸ Index: {index_name}")
    print()

    index = create_or_get_index(index_name, force_recreate=force_reload)

    if not force_reload:
        existing_sources = get_existing_sources_from_index(index_name)
        print(f"ğŸ“Š File Ä‘Ã£ cÃ³ trong Index: {len(existing_sources)}")
    else:
        existing_sources = set()

    target_files = {os.path.basename(p): p for p in json_paths}

    if force_reload:
        files_to_load = target_files
    else:
        files_to_load = {
            n: p for n, p in target_files.items()
            if n not in existing_sources
        }

    print(f"ğŸ“¥ Sáº½ náº¡p {len(files_to_load)} file má»›i.\n")

    all_docs = []
    file_stats = {}

    for filename, path in files_to_load.items():
        print(f"ğŸ“„ {filename}...", end=" ")
        docs = load_and_chunk_json(path)

        if docs:
            all_docs.extend(docs)
            file_stats[filename] = len(docs)
            print(f"âœ“ {len(docs)} docs")
        else:
            print("âœ— Lá»—i")

    if not all_docs:
        print("âŒ KhÃ´ng cÃ³ document nÃ o Ä‘á»ƒ náº¡p!")
        return

    print(f"\nğŸ“¦ Tá»•ng cá»™ng {len(all_docs)} docs\n")

    print("ğŸ’¾ Äang náº¡p vÃ o Pinecone...\n")
    total_batches = (len(all_docs) + BATCH_SIZE - 1) // BATCH_SIZE
    vectordb = None

    try:
        for i in range(0, len(all_docs), BATCH_SIZE):
            batch_docs = all_docs[i:i+BATCH_SIZE]
            batch_num = (i // BATCH_SIZE) + 1

            print(f"   ğŸ“¦ Batch {batch_num}/{total_batches} ({len(batch_docs)} docs)...", end=" ")

            if i == 0:
                vectordb = Pinecone.from_texts(
                    texts=[doc["text"] for doc in batch_docs],
                    metadatas=[doc["metadata"] for doc in batch_docs],
                    embedding=emb,
                    index_name=index_name
                )
            else:
                vectordb.add_texts(
                    texts=[doc["text"] for doc in batch_docs],
                    metadatas=[doc["metadata"] for doc in batch_docs]
                )

            print("âœ“")
            time.sleep(1)

    except Exception as e:
        print(f"\nâŒ Lá»—i khi náº¡p vÃ o Pinecone: {e}")
        return

    stats = index.describe_index_stats()

    print("\n" + "="*70)
    print("ğŸ“Š Káº¾T QUáº¢ CUá»I")
    print("="*70)
    print(f"   âœ“ Tá»•ng vectors: {stats['total_vector_count']}")
    print(f"   âœ“ File xá»­ lÃ½: {len(file_stats)}")
    for filename, ct in file_stats.items():
        print(f"   â€¢ {filename}: {ct} docs")
    print("="*70)


# ===================== MAIN =====================
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Náº¡p file JSON vÃ o Pinecone Index"
    )
    parser.add_argument(
        "--force-reload",
        action="store_true",
        help="XÃ³a vÃ  náº¡p láº¡i toÃ n bá»™ Index"
    )
    parser.add_argument(
        "--folder",
        type=str,
        default=JSON_FOLDER,
        help=f"ÄÆ°á»ng dáº«n folder chá»©a JSON (máº·c Ä‘á»‹nh: {JSON_FOLDER})"
    )

    args = parser.parse_args()

    json_files = get_json_files_from_folder(args.folder)

    if not json_files:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y file JSON nÃ o.")
        exit(1)

    print(f"ğŸ“„ TÃ¬m tháº¥y {len(json_files)} file JSON:")
    for i, fpath in enumerate(json_files, 1):
        print(f"   {i}. {os.path.basename(fpath)}")
    print()

    ingest_documents_to_pinecone(
        json_paths=json_files,
        index_name=PINECONE_INDEX_NAME,
        force_reload=args.force_reload
    )

    print("\nğŸ‰ HOÃ€N THÃ€NH!")
